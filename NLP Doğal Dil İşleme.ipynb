{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac5115d",
   "metadata": {},
   "source": [
    "## NLP-Natural Language Processing(Doğal Dil İşleme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa306b7e",
   "metadata": {},
   "source": [
    "### Kütüphaneleri yükle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec68cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081fe8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0344b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/silaorak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f10f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbd2d5",
   "metadata": {},
   "source": [
    "### STR Metotları "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea8243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='BTK Akademi Antalya ileri düzey makine öğrenmesi atölyesi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12aed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Btk Akademi Antalya Ileri Düzey Makine Öğrenmesi Atölyesi'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.title() #tüm sözcüklerin ilk harfi büyük"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48853b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Btk akademi antalya ileri düzey makine öğrenmesi atölyesi'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.capitalize() #ilk harfi büyük yapar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4af9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTK AKADEMI ANTALYA ILERI DÜZEY MAKINE ÖĞRENMESI ATÖLYESI'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.upper() #tüm harfler büyük"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446cfda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTK Akademi Antalya ileri düzey makine öğrenmesi atölyesi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3919c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTK AKADEMI ANTALYA ILERI DÜZEY MAKINE ÖĞRENMESI ATÖLYESI'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=text.upper()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d21c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'btk akademi antalya ileri düzey makine öğrenmesi atölyesi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower() #tüm harfleri kücük harfe cevirir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba0ac55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTK',\n",
       " 'AKADEMI',\n",
       " 'ANTALYA',\n",
       " 'ILERI',\n",
       " 'DÜZEY',\n",
       " 'MAKINE',\n",
       " 'ÖĞRENMESI',\n",
       " 'ATÖLYESI']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sozcuk_liste=text.split()#varsayılan olarak boşluklardan ayrılır bir liste döndürür\n",
    "sozcuk_liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "439f3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTK AKADEMI ANTALYA ILERI DÜZEY MAKINE ÖĞRENMESI ATÖLYESI\n"
     ]
    }
   ],
   "source": [
    "print(*sozcuk_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa7d7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65e310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3ccbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2='Merhaba BTK Akademi kursuna hoşgeldiniz.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbcf2b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Merhaba BTK Akademi kursuna hoşgeldiniz.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text2) #cümle bazında ayırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04957e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Merhaba', 'BTK', 'Akademi', 'kursuna', 'hoşgeldiniz', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text2) #kelime bazında ayırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b08a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46871c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text='''\n",
    "Merhaba hocam\n",
    "Nasılsınız?\n",
    "Sınavdan 90 aldım. 100 bekliyordum.\n",
    "Sınav kağıdıma tekrar bakarsanız sevinirim. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce93fba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Merhaba',\n",
       " 'hocam',\n",
       " 'Nasılsınız',\n",
       " '?',\n",
       " 'Sınavdan',\n",
       " '90',\n",
       " 'aldım',\n",
       " '.',\n",
       " '100',\n",
       " 'bekliyordum',\n",
       " '.',\n",
       " 'Sınav',\n",
       " 'kağıdıma',\n",
       " 'tekrar',\n",
       " 'bakarsanız',\n",
       " 'sevinirim',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21c20f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nMerhaba hocam\\nNasılsınız?\\nSınavdan 90 aldım. 100 bekliyordum.\\nSınav kağıdıma tekrar bakarsanız sevinirim. ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f28a87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merhaba hocam\n",
      "Nasılsınız?\n",
      "Sınavdan 90 aldım. 100 bekliyordum.\n",
      "Sınav kağıdıma tekrar bakarsanız sevinirim. \n"
     ]
    }
   ],
   "source": [
    "print('\\nMerhaba hocam\\nNasılsınız?\\nSınavdan 90 aldım. 100 bekliyordum.\\nSınav kağıdıma tekrar bakarsanız sevinirim. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b20f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1029ac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', ' ', '\\n', '\\n', ' ', ' ', ' ', ' ', '\\n', ' ', ' ', ' ', ' ', ' ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexpTokenizer('\\s+').tokenize(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4fb8508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Merhaba',\n",
       " 'hocam',\n",
       " 'Nasılsınız?',\n",
       " 'Sınavdan',\n",
       " '90',\n",
       " 'aldım.',\n",
       " '100',\n",
       " 'bekliyordum.',\n",
       " 'Sınav',\n",
       " 'kağıdıma',\n",
       " 'tekrar',\n",
       " 'bakarsanız',\n",
       " 'sevinirim.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexpTokenizer('\\s+',gaps=True).tokenize(sent_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8ad50",
   "metadata": {},
   "source": [
    "RegexpTokenizer() fonksiyonu, bir tokenizasyon (parçalama) işlemi için kullanılacak bir nesne oluşturur.\n",
    "İlk parametre olan '\\s+' bir düzenli ifadedir (regular expression). Bu ifade, bir veya daha fazla boşluk karakterini (boşluk, tab, satır başı vb.) eşleştirir.\n",
    "gaps=True parametresi, tokenizasyon işleminin boşluk karakterleri arasındaki metinleri parçalaması gerektiğini belirtir. Yani, boşluk karakterleri değil, boşluk karakterleri arasındaki metinler token olarak alınır.\n",
    "tokenize() metodu, sent_text değişkenindeki metni bu kurallara göre parçalar ve elde edilen token'ları bir liste halinde döndürür.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a241c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='She secures 90% in class Xz In. She is a Meritorius student.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "577ddd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'Xz', 'In', 'She', 'Meritorius']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap=RegexpTokenizer('[A-Z]\\w+')\n",
    "#[A-Z] büyük harfleri belirtir büyük harflerin  \n",
    "#\\w+ ardından gelen 1 veya daha fazla alfanumeric karakter \n",
    "cap.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8582e",
   "metadata": {},
   "source": [
    "### Ek ve kök işlemleri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cddc427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pr=PorterStemmer()\n",
    "pr.stem('Running') #girilen kelimenin kökünü bulur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91e1be3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.stem('talking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9866671",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['houses', 'trains','pens','cars', 'eaten','sick', 'nice', 'bought', 'selling', 'sized',\n",
    "      'speech', 'rolling', 'marching','identification','universal','beatiful','references' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a42bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hous', 'train', 'pen', 'car', 'eaten', 'sick', 'nice', 'bought', 'sell', 'size', 'speech', 'roll', 'march', 'identif', 'univers', 'beati', 'refer']\n"
     ]
    }
   ],
   "source": [
    "kok=[pr.stem(word) for word in words] #list comperehension ile kısaltılmış hali\n",
    "print(kok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d205a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# List Comprehension\n",
    "sayilar=list(range(10))\n",
    "print(sayilar)\n",
    "sayilar2=[sayi**2 for sayi in sayilar]\n",
    "print(sayilar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c402dfd",
   "metadata": {},
   "source": [
    "Stemming'de, kelimelerin çekim ve türetme eklerini mekaniksel olarak çıkartılırken, lemmatizasyon daha akıllıca ve dil bilgisi temelli bir yaklaşım kullanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "046a7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff277d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/silaorak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebb218e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singing'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('singing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf375c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('singing', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e4cb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singing'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('singing', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a08a9c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'take'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('took', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b54ec056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /opt/anaconda3/lib/python3.11/site-packages (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect #otomatik düzeltme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d34f4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5affe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell=Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f72d4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my spelling is good'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('my spelling is goad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ce40d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.11/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/anaconda3/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38d343b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8c04aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have a good book\")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=TextBlob('I fave a goad book')\n",
    "b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfe5a676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TextBlob in module textblob.blob:\n",
      "\n",
      "class TextBlob(BaseBlob)\n",
      " |  TextBlob(text, tokenizer=None, pos_tagger=None, np_extractor=None, analyzer=None, parser=None, classifier=None, clean_html=False)\n",
      " |  \n",
      " |  A general text block, meant for larger bodies of text (esp. those\n",
      " |  containing sentences). Inherits from :class:`BaseBlob <BaseBlob>`.\n",
      " |  \n",
      " |  :param str text: A string.\n",
      " |  :param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to\n",
      " |      :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.\n",
      " |  :param np_extractor: (optional) An NPExtractor instance. If ``None``,\n",
      " |      defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.\n",
      " |  :param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to\n",
      " |      :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.\n",
      " |  :param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to\n",
      " |      :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.\n",
      " |  :param classifier: (optional) A classifier.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TextBlob\n",
      " |      BaseBlob\n",
      " |      textblob.mixins.StringlikeMixin\n",
      " |      textblob.mixins.BlobComparableMixin\n",
      " |      textblob.mixins.ComparableMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  sentences = <textblob.decorators.cached_property object>\n",
      " |      Return list of :class:`Sentence <Sentence>` objects.\n",
      " |  \n",
      " |  to_json(self, *args, **kwargs)\n",
      " |      Return a json representation (str) of this blob.\n",
      " |      Takes the same arguments as json.dumps.\n",
      " |      \n",
      " |      .. versionadded:: 0.5.1\n",
      " |  \n",
      " |  words = <textblob.decorators.cached_property object>\n",
      " |      Return a list of word tokens. This excludes punctuation characters.\n",
      " |      If you want to include punctuation characters, access the ``tokens``\n",
      " |      property.\n",
      " |      \n",
      " |      :returns: A :class:`WordList <WordList>` of word tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  json\n",
      " |      The json representation of this blob.\n",
      " |      \n",
      " |      .. versionchanged:: 0.5.1\n",
      " |          Made ``json`` a property instead of a method to restore backwards\n",
      " |          compatibility that was broken after version 0.4.0.\n",
      " |  \n",
      " |  raw_sentences\n",
      " |      List of strings, the raw sentences in the blob.\n",
      " |  \n",
      " |  serialized\n",
      " |      Returns a list of each sentence's dict representation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseBlob:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Concatenates two text objects the same way Python strings are\n",
      " |      concatenated.\n",
      " |      \n",
      " |      Arguments:\n",
      " |      - `other`: a string or a text object\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, text, tokenizer=None, pos_tagger=None, np_extractor=None, analyzer=None, parser=None, classifier=None, clean_html=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  classify(self)\n",
      " |      Classify the blob using the blob's ``classifier``.\n",
      " |  \n",
      " |  correct(self)\n",
      " |      Attempt to correct the spelling of a blob.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.0\n",
      " |      \n",
      " |      :rtype: :class:`BaseBlob <BaseBlob>`\n",
      " |  \n",
      " |  ngrams(self, n=3)\n",
      " |      Return a list of n-grams (tuples of n successive words) for this\n",
      " |      blob.\n",
      " |      \n",
      " |      :rtype: List of :class:`WordLists <WordList>`\n",
      " |  \n",
      " |  noun_phrases = <textblob.decorators.cached_property object>\n",
      " |      Returns a list of noun phrases for this blob.\n",
      " |  \n",
      " |  np_counts = <textblob.decorators.cached_property object>\n",
      " |      Dictionary of noun phrase frequencies in this text.\n",
      " |  \n",
      " |  parse(self, parser=None)\n",
      " |      Parse the text.\n",
      " |      \n",
      " |      :param parser: (optional) A parser instance. If ``None``, defaults to\n",
      " |          this blob's default parser.\n",
      " |      \n",
      " |      .. versionadded:: 0.6.0\n",
      " |  \n",
      " |  polarity = <textblob.decorators.cached_property object>\n",
      " |      Return the polarity score as a float within the range [-1.0, 1.0]\n",
      " |      \n",
      " |      :rtype: float\n",
      " |  \n",
      " |  pos_tags = <textblob.decorators.cached_property object>\n",
      " |      Returns an list of tuples of the form (word, POS tag).\n",
      " |      \n",
      " |      Example:\n",
      " |      ::\n",
      " |      \n",
      " |          [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
      " |                  ('Thursday', 'NNP'), ('morning', 'NN')]\n",
      " |      \n",
      " |      :rtype: list of tuples\n",
      " |  \n",
      " |  sentiment = <textblob.decorators.cached_property object>\n",
      " |      Return a tuple of form (polarity, subjectivity ) where polarity\n",
      " |      is a float within the range [-1.0, 1.0] and subjectivity is a float\n",
      " |      within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is\n",
      " |      very subjective.\n",
      " |      \n",
      " |      :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)``\n",
      " |  \n",
      " |  sentiment_assessments = <textblob.decorators.cached_property object>\n",
      " |      Return a tuple of form (polarity, subjectivity, assessments ) where\n",
      " |      polarity is a float within the range [-1.0, 1.0], subjectivity is a\n",
      " |      float within the range [0.0, 1.0] where 0.0 is very objective and 1.0\n",
      " |      is very subjective, and assessments is a list of polarity and\n",
      " |      subjectivity scores for the assessed tokens.\n",
      " |      \n",
      " |      :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity,\n",
      " |      assessments)``\n",
      " |  \n",
      " |  split(self, sep=None, maxsplit=9223372036854775807)\n",
      " |      Behaves like the built-in str.split() except returns a\n",
      " |      WordList.\n",
      " |      \n",
      " |      :rtype: :class:`WordList <WordList>`\n",
      " |  \n",
      " |  subjectivity = <textblob.decorators.cached_property object>\n",
      " |      Return the subjectivity score as a float within the range [0.0, 1.0]\n",
      " |      where 0.0 is very objective and 1.0 is very subjective.\n",
      " |      \n",
      " |      :rtype: float\n",
      " |  \n",
      " |  tags = <textblob.decorators.cached_property object>\n",
      " |      Returns an list of tuples of the form (word, POS tag).\n",
      " |      \n",
      " |      Example:\n",
      " |      ::\n",
      " |      \n",
      " |          [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
      " |                  ('Thursday', 'NNP'), ('morning', 'NN')]\n",
      " |      \n",
      " |      :rtype: list of tuples\n",
      " |  \n",
      " |  tokenize(self, tokenizer=None)\n",
      " |      Return a list of tokens, using ``tokenizer``.\n",
      " |      \n",
      " |      :param tokenizer: (optional) A tokenizer object. If None, defaults to\n",
      " |          this blob's default tokenizer.\n",
      " |  \n",
      " |  tokens = <textblob.decorators.cached_property object>\n",
      " |      Return a list of tokens, using this blob's tokenizer object\n",
      " |      (defaults to :class:`WordTokenizer <textblob.tokenizers.WordTokenizer>`).\n",
      " |  \n",
      " |  word_counts = <textblob.decorators.cached_property object>\n",
      " |      Dictionary of word frequencies in this text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseBlob:\n",
      " |  \n",
      " |  analyzer = <textblob.en.sentiments.PatternAnalyzer object>\n",
      " |  \n",
      " |  np_extractor = <textblob.en.np_extractors.FastNPExtractor object>\n",
      " |  \n",
      " |  parser = <textblob.en.parsers.PatternParser object>\n",
      " |  \n",
      " |  pos_tagger = <textblob.en.taggers.NLTKTagger object>\n",
      " |  \n",
      " |  tokenizer = <textblob.tokenizers.WordTokenizer object>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from textblob.mixins.StringlikeMixin:\n",
      " |  \n",
      " |  __contains__(self, sub)\n",
      " |      Implements the `in` keyword like a Python string.\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns a  substring. If index is an integer, returns a Python\n",
      " |      string of a single character. If a range is given, e.g. `blob[3:5]`,\n",
      " |      a new instance of the class is returned.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Makes the object iterable as if it were a string,\n",
      " |      iterating through the raw string's characters.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the length of the raw text.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns a string representation for debugging.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Returns a string representation used in print statements\n",
      " |      or str(my_blob).\n",
      " |  \n",
      " |  ends_with = endswith(self, suffix, start=0, end=9223372036854775807)\n",
      " |  \n",
      " |  endswith(self, suffix, start=0, end=9223372036854775807)\n",
      " |      Returns True if the blob ends with the given suffix.\n",
      " |  \n",
      " |  find(self, sub, start=0, end=9223372036854775807)\n",
      " |      Behaves like the built-in str.find() method. Returns an integer,\n",
      " |      the index of the first occurrence of the substring argument sub in the\n",
      " |      sub-string given by [start:end].\n",
      " |  \n",
      " |  format(self, *args, **kwargs)\n",
      " |      Perform a string formatting operation, like the built-in\n",
      " |      `str.format(*args, **kwargs)`. Returns a blob object.\n",
      " |  \n",
      " |  index(self, sub, start=0, end=9223372036854775807)\n",
      " |      Like blob.find() but raise ValueError when the substring\n",
      " |      is not found.\n",
      " |  \n",
      " |  join(self, iterable)\n",
      " |      Behaves like the built-in `str.join(iterable)` method, except\n",
      " |      returns a blob object.\n",
      " |      \n",
      " |      Returns a blob which is the concatenation of the strings or blobs\n",
      " |      in the iterable.\n",
      " |  \n",
      " |  lower(self)\n",
      " |      Like str.lower(), returns new object with all lower-cased characters.\n",
      " |  \n",
      " |  replace(self, old, new, count=9223372036854775807)\n",
      " |      Return a new blob object with all the occurence of `old` replaced\n",
      " |      by `new`.\n",
      " |  \n",
      " |  rfind(self, sub, start=0, end=9223372036854775807)\n",
      " |      Behaves like the built-in str.rfind() method. Returns an integer,\n",
      " |      the index of he last (right-most) occurence of the substring argument\n",
      " |      sub in the sub-sequence given by [start:end].\n",
      " |  \n",
      " |  rindex(self, sub, start=0, end=9223372036854775807)\n",
      " |      Like blob.rfind() but raise ValueError when substring is not\n",
      " |      found.\n",
      " |  \n",
      " |  starts_with = startswith(self, prefix, start=0, end=9223372036854775807)\n",
      " |  \n",
      " |  startswith(self, prefix, start=0, end=9223372036854775807)\n",
      " |      Returns True if the blob starts with the given prefix.\n",
      " |  \n",
      " |  strip(self, chars=None)\n",
      " |      Behaves like the built-in str.strip([chars]) method. Returns\n",
      " |      an object with leading and trailing whitespace removed.\n",
      " |  \n",
      " |  title(self)\n",
      " |      Returns a blob object with the text in title-case.\n",
      " |  \n",
      " |  upper(self)\n",
      " |      Like str.upper(), returns new object with all upper-cased characters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from textblob.mixins.StringlikeMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from textblob.mixins.ComparableMixin:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TextBlob) #metotlar hakkında bilgi almak için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8e13802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ccd68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d107835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect('Merhaba arkadaşlar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1dced63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect('Ich bin ein Berliner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f89cdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seni seviyorum'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googletrans\n",
    "from googletrans import Translator\n",
    "translator=Translator()\n",
    "text='I love you'\n",
    "text=translator.translate(text, dest='tr')\n",
    "text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e38cd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text:Burada olduğum için mutluyum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am happy to be here'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=input('Enter text:')\n",
    "text=translator.translate(text, dest='en')\n",
    "text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ece53e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c74cfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "uccumle=['call you tonight', 'Call me a cab', 'call me please call please']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99dd83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f62efb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04555c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=pd.DataFrame(vect.fit_transform(uccumle).toarray(),\n",
    "                columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9bd9f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     2   1       2        0    0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf #kelimelerin cümlelerde geçme sayısını verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f1068bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=vect.transform(uccumle).toarray().sum(axis=0) #sütunların toplamını verir\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd4ec203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call   me  please  tonight  you\n",
       "0  0.0  0.25  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.25  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.50  0.5     1.0      0.0  0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf/df #oransal olarak cümlelere dağılımını verir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
